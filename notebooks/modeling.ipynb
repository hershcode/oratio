{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujlV8w27gTq5"
      },
      "source": [
        "- [Imports for Data Preparation](#Data-Preparation)\n",
        "- [Modeling](##Deep-Learning-Models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO0rhhohgTq9"
      },
      "source": [
        "#### General Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IWPAzkGUgTq9"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "from scipy.io import wavfile as wav\n",
        "from sklearn import metrics \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split \n",
        "import tensorflow as tf\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "PATH = os.getcwd() + '/speech_commands_v0.01/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0Ns2KuaogTq_"
      },
      "outputs": [],
      "source": [
        "def get_directory_contents(path):\n",
        "    return os.listdir(path)\n",
        "\n",
        "def open_file(filename):\n",
        "    \n",
        "    f = open(filename)\n",
        "    return f.read().splitlines()\n",
        "\n",
        "def compile_dataset(folders):\n",
        "    \n",
        "    total_words = []\n",
        "    for folder in folders:\n",
        "        words = get_directory_contents(path=PATH+folder)\n",
        "        words = [folder+'/'+word for word in words]\n",
        "        total_words = total_words + words\n",
        "    \n",
        "    dataset = create_df(words=total_words)\n",
        "                           \n",
        "    return dataset\n",
        "\n",
        "def create_df(words):\n",
        "    \n",
        "    data = pd.DataFrame({'recordings':words})\n",
        "\n",
        "    data['word'] = data['recordings'].str.split('/').str[0]\n",
        "    data['speaker_id'] = data['recordings'].str.split('/').str[1]\n",
        "    data['speaker_id'] =data['speaker_id'].str.split('_').str[0]\n",
        "    \n",
        "    return data\n",
        "\n",
        "def summary(data):\n",
        "    \n",
        "    summary_df = pd.DataFrame()\n",
        "    summary_df['total_recordings'] = [data.shape[0]]\n",
        "    summary_df['total_speakers'] = len(data['speaker_id'].unique())\n",
        "    summary_df['total_words'] = len(data['word'].unique())\n",
        "    \n",
        "    return summary_df\n",
        "\n",
        "def word_distribution():\n",
        "    \n",
        "    word_count = data['word'].value_counts()\n",
        "    ax = word_count.plot(kind='bar', figsize=(8,4), alpha=0.5)\n",
        "    plt.show()\n",
        "    \n",
        "def extract_features(file_name):\n",
        "    audio, sample_rate = librosa.load(file_name) \n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "    mfccs_processed = np.mean(mfccs.T,axis=0)\n",
        "     \n",
        "    return mfccs_processed\n",
        "\n",
        "def dataset_prep(df):\n",
        "    master_df = pd.DataFrame()\n",
        "    columns = ['mfc_{}'.format(i) for i in range(40)]\n",
        "    # Iterate through each sound file and extract the features \n",
        "    for index, row in df.iterrows():\n",
        "        file_name = 'speech_commands_v0.01/'+row['recordings']\n",
        "        class_label = row[\"word\"]\n",
        "        data = extract_features(file_name).reshape(1,-1)\n",
        "        temp_df = pd.DataFrame(data, columns=columns)\n",
        "        temp_df['class_label'] = class_label \n",
        "        master_df = master_df.append(temp_df, ignore_index=True)\n",
        "\n",
        "    return master_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EXOWTWSgTrC"
      },
      "source": [
        "### Data preparation\n",
        "#### Only run if dataset not stored in csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgqrfzNGgTrE"
      },
      "outputs": [],
      "source": [
        "remove_words = [\n",
        "    '.DS_Store', 'validation_list.txt', 'LICENSE',\n",
        "    '_background_noise_', 'README.md', 'testing_list.txt'\n",
        "]\n",
        "\n",
        "names = get_directory_contents(path=PATH)\n",
        "names = [word for word in names if word not in remove_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJKLn6LbgTrE"
      },
      "outputs": [],
      "source": [
        "val_list = open_file(filename=PATH+'validation_list.txt')\n",
        "val_df = create_df(words=val_list)\n",
        "test_list = open_file(filename=PATH+'testing_list.txt')\n",
        "test_df = create_df(words=test_list)\n",
        "total_df = compile_dataset(folders=names)\n",
        "\n",
        "training_df = total_df[~total_df['recordings'].isin(val_df['recordings'])]\n",
        "training_df = training_df[~training_df['recordings'].isin(test_df['recordings'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtYitUihgTrF"
      },
      "outputs": [],
      "source": [
        "featuresdf = dataset_prep(df=training_df)\n",
        "testdf = dataset_prep(df=test_df)\n",
        "valdf = dataset_prep(df=val_df)\n",
        "featuresdf.to_csv(\"training_set.csv\")\n",
        "testdf.to_csv(\"test_set.csv\")\n",
        "valdf.to_csv(\"validation_set.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnLuit7lgTrF"
      },
      "source": [
        "### To read from CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MNz3hUebgTrG"
      },
      "outputs": [],
      "source": [
        "featuresdf = pd.read_csv(\"training_set.csv\", index_col=0)\n",
        "testdf = pd.read_csv(\"test_set.csv\", index_col=0)\n",
        "valdf = pd.read_csv(\"validation_set.csv\", index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHOnP17zgTrG"
      },
      "source": [
        "### Modeling preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VAkzhs0bgTrH"
      },
      "outputs": [],
      "source": [
        "# Convert features and corresponding classification labels into numpy arrays\n",
        "X = np.array(featuresdf.drop(labels='class_label', axis=1).values)\n",
        "y = np.array(featuresdf.class_label.tolist())\n",
        "# Encode the classification labels\n",
        "le = LabelEncoder()\n",
        "yy = tf.keras.utils.to_categorical(le.fit_transform(y))\n",
        "\n",
        "x_val = np.array(valdf.drop(labels='class_label', axis=1).values)\n",
        "y_val = np.array(valdf.class_label.tolist())\n",
        "y_val = tf.keras.utils.to_categorical(le.transform(y_val))\n",
        "\n",
        "x_test = np.array(testdf.drop(labels='class_label', axis=1).values)\n",
        "y_test = np.array(testdf.class_label.tolist())\n",
        "y_test = tf.keras.utils.to_categorical(le.transform(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "yRuNqKXSAq81"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X)\n",
        "X_val_sacled = scaler.transform(x_val)\n",
        "X_test_scaled = scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "wZHL40dQApYH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkV3OQNQgTrH"
      },
      "source": [
        "### Normal Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aapjDh4gTrI"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression(multi_class='multinomial', solver='newton-cg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfJ4gMDDgTrJ"
      },
      "outputs": [],
      "source": [
        "def to_category(columns, dataframe):\n",
        "    \"\"\"Convert a list of columns, from a dataframe, to a category datatype\"\"\"\n",
        "    for column in columns: \n",
        "        dataframe[column] = dataframe[column].astype('category')\n",
        "    return dataframe\n",
        "columns=['word']\n",
        "\n",
        "training_df = to_category(columns=columns, dataframe=training_df)\n",
        "test_df = to_category(columns=columns, dataframe=test_df)\n",
        "val_df = to_category(columns=columns, dataframe=val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9PNbpnbgTrK"
      },
      "outputs": [],
      "source": [
        "clf.fit(X, training_df['word'])\n",
        "\n",
        "print(\"Training Accuracy: {0:.2%}\".format(clf.score(X, training_df['word'])))\n",
        "print(\"Validation Accuracy: {0:.2%}\".format(clf.score(x_val, val_df['word'])))\n",
        "print(\"Testing Accuracy: {0:.2%}\".format(clf.score(x_test, test_df['word'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxdKjJjjgTrK"
      },
      "source": [
        "## Deep Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "lT_1X729gTrK"
      },
      "outputs": [],
      "source": [
        "def build_model_graph(input_shape=(13,)):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(input_shape),\n",
        "        tf.keras.layers.Dense(100, activation='relu'),\n",
        "        tf.keras.layers.Dropout(.1, input_shape=(100,)),\n",
        "        tf.keras.layers.Dense(300, activation='relu'),\n",
        "        tf.keras.layers.Dropout(.15, input_shape=(300,)),\n",
        "        tf.keras.layers.Dense(800, activation='relu'),\n",
        "        tf.keras.layers.Dropout(.2, input_shape=(800,)),\n",
        "        tf.keras.layers.Dense(2000, activation='relu'),\n",
        "        tf.keras.layers.Dropout(.3, input_shape=(2000,)),\n",
        "        tf.keras.layers.Dense(5500, activation='relu'),\n",
        "        tf.keras.layers.Dropout(.4, input_shape=(5500,)),\n",
        "        tf.keras.layers.Dense(1000, activation='relu'),\n",
        "        tf.keras.layers.Dense(500, activation='relu'),\n",
        "        tf.keras.layers.Dense(250, activation='relu'),\n",
        "        tf.keras.layers.Dense(100, activation='relu'),\n",
        "        tf.keras.layers.Dense(30, activation='softmax')\n",
        "    ])\n",
        "    # Compile the model\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.0013\n",
        "    )\n",
        "    model.compile(loss='categorical_crossentropy', \n",
        "                  metrics=['accuracy'], optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "def plot_metric(history, metric):\n",
        "    train_metrics = history.history[metric]\n",
        "    val_metrics = history.history['val_'+metric]\n",
        "    epochs = range(1, len(train_metrics) + 1)\n",
        "    plt.plot(epochs, train_metrics)\n",
        "    plt.plot(epochs, val_metrics)\n",
        "    plt.title('Training and validation '+ metric)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "aJKaJmlogTrK"
      },
      "outputs": [],
      "source": [
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss')\n",
        "lr = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10 ** (epoch / 30))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PxN-6rxgTrL"
      },
      "source": [
        "- Build out different sections within the notebook\n",
        "    - Give a summary for each section\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "DSa24E-DgTrL",
        "outputId": "ae5ec393-b5d4-4c60-aeeb-4fac34857be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 285/1597 [====>.........................] - ETA: 3s - loss: 3.1756 - accuracy: 0.0830"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2dace73a1881>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_sacled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                           \u001b[0;31m# callbacks =lr,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                           verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m             with tf.profiler.experimental.Trace(\n\u001b[1;32m   1378\u001b[0m                 \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1244\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m       \u001b[0moriginal_spe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1247\u001b[0m       can_run_full_execution = (\n\u001b[1;32m   1248\u001b[0m           \u001b[0moriginal_spe\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    676\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCompositeTensor\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0minput\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m   \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompositeTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"graph\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_register\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = build_model_graph()\n",
        "\n",
        "model_history = model.fit(x=X_train_scaled, \n",
        "                          y=yy, \n",
        "                          epochs=100, \n",
        "                          validation_data=(X_val_sacled, y_val),\n",
        "                          # callbacks =lr,\n",
        "                          verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdFIdHKJgTrL"
      },
      "outputs": [],
      "source": [
        "\"plot_metric(history=model_history, metric='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00ZF58XCgTrL"
      },
      "outputs": [],
      "source": [
        "score = wide_nn.evaluate(x_test, y_test, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL0WHCkQgTrM"
      },
      "outputs": [],
      "source": [
        "score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ebcFSuIgTrM"
      },
      "source": [
        "- Increase epochs til loss stabalizes\n",
        "- Try different activation and optimizers\n",
        "- Add dropout\n",
        "- Have variability in diverging and convering with hidden layer units"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KQSsrF2gTrM"
      },
      "source": [
        "- Unsupervised Learning\n",
        "Concept of what works best for the problem we're solving\n",
        "\n",
        "autoregression, vector auto regression\n",
        "\n",
        "decisiontree regression, support vector regression, autocorrelation "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "general",
      "language": "python",
      "name": "general"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}